ğŸ” Quâ€™est-ce quâ€™un Vision Transformer (ViT-B) ?
Le Vision Transformer (ViT) est un modÃ¨le dâ€™apprentissage profond spÃ©cialement conÃ§u pour traiter des images en utilisant lâ€™architecture des transformers (comme en NLP avec BERT ou GPT).
Il a Ã©tÃ© introduit par Google dans lâ€™article :
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (2020)

ğŸ“– Principe gÃ©nÃ©ral du ViT :
Contrairement aux CNN classiques (comme ResNet), ViT ne travaille pas sur des pixels entiers ou des convolutions, mais :

DÃ©coupe lâ€™image en petits patchs (par exemple des morceaux de 16x16 pixels)

Aplati chaque patch et le transforme en un vecteur (embedding)

Ajoute un "position embedding" pour ne pas perdre la localisation spatiale

Envoie cette sÃ©quence de vecteurs dans un Transformer Encoder (comme en NLP)

Produit une classe prÃ©dite Ã  la fin (classification ImageNet dans ton cas)

ğŸ“¦ Dans ton cas - ViT-B (Base) :
Base = la taille du modÃ¨le (86M de paramÃ¨tres environ)

Patch size = 16x16 pixels

EntrÃ©e image = 224x224 pixels

Output = Une des 1000 classes ImageNet (chat, voiture, avion, etc.)
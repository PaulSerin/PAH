{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINETUNING da la derniere couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [02:18<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss moyenne: 0.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [02:18<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss moyenne: 0.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [02:18<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss moyenne: 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:41<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sur CIFAR-10: 97.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fonction de collate personnalisée pour garder les images sous forme de liste PIL\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), torch.tensor(labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Préparer le dataset CIFAR-10 avec transformation (redimensionnement à 224x224)\n",
    "transform = transforms.Resize((224, 224))\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Réduire le train set à 30% de ses données\n",
    "total_train = len(train_dataset)\n",
    "train_size = int(0.3 * total_train)\n",
    "train_subset, _ = random_split(train_dataset, [train_size, total_train - train_size])\n",
    "\n",
    "# Créer les DataLoaders en utilisant la fonction de collate personnalisée\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Charger le processor et le modèle ViT pré-entraîné\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "# Remplacer la couche finale pour qu'elle produise 10 classes au lieu de 1000\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 10)\n",
    "model.to(device)\n",
    "\n",
    "# Définir la loss et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # Le processor attend une liste d'images PIL\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss moyenne: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct += (preds == labels.to(device)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(\"Accuracy sur CIFAR-10: {:.2f}%\".format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINETUNING des 3 dernieres couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:24<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss moyenne: 0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:24<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss moyenne: 0.0626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:24<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss moyenne: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:40<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sur CIFAR-10: 97.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fonction de collate personnalisée pour garder les images sous forme de liste PIL\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), torch.tensor(labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Préparer le dataset CIFAR-10 avec transformation (redimensionnement à 224x224)\n",
    "transform = transforms.Resize((224, 224))\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Réduire le train set à 30% de ses données pour un test rapide\n",
    "total_train = len(train_dataset)\n",
    "train_size = int(0.1 * total_train)\n",
    "train_subset, _ = random_split(train_dataset, [train_size, total_train - train_size])\n",
    "\n",
    "# Créer les DataLoaders en utilisant la fonction de collate personnalisée\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Charger le processor et le modèle ViT pré-entraîné\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# ----------------- Fine-Tuning sur les dernières couches -----------------\n",
    "# 1. Geler tout d'abord le backbone\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Débloquer (unfreeze) les derniers blocs du transformeur.\n",
    "# Ici, on choisit de réentraîner les 2 derniers blocs\n",
    "for layer in model.vit.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3. Remplacer la tête de classification pour 10 classes (pour CIFAR-10)\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 10)\n",
    "# Par défaut, les paramètres de cette nouvelle couche sont entraînables\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Définir la loss et l'optimiseur.\n",
    "# L'optimiseur portera sur tous les paramètres non gelés (les 2 derniers blocs et le classifier)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Boucle d'entraînement\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # Préparation des inputs via le processor (attend une liste d'images PIL)\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss moyenne: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Évaluation sur le jeu de test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct += (preds == labels.to(device)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(\"Accuracy sur CIFAR-10: {:.2f}%\".format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du modèle baseline: 343.29 MB\n"
     ]
    }
   ],
   "source": [
    "size_baseline = get_model_size(model)\n",
    "print(\"Taille du modèle baseline: {:.2f} MB\".format(size_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Quantized:   0%|                                                                                                                                                    | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Quantized: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [03:32<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Résultats - QUANTIZATION (tête uniquement)\n",
      "Accuracy : 97.24%\n",
      "Taille du modèle : 343.29 MB\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Copier le modèle fine-tuné\n",
    "model_quantized = copy.deepcopy(model)\n",
    "\n",
    "# Quantization dynamique du classifier\n",
    "model_quantized.classifier = torch.quantization.quantize_dynamic(\n",
    "    model_quantized.classifier, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "model_quantized.to(\"cpu\")\n",
    "\n",
    "# Évaluation du modèle quantized\n",
    "correct_q, total_q = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluation Quantized\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        outputs = model_quantized(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct_q += (preds == labels).sum().item()\n",
    "        total_q += labels.size(0)\n",
    "\n",
    "accuracy_quantized = 100 * correct_q / total_q\n",
    "size_quantized = get_model_size(model_quantized)\n",
    "\n",
    "print(\"\\n⚡ Résultats - QUANTIZATION (tête uniquement)\")\n",
    "print(f\"Accuracy : {accuracy_quantized:.2f}%\")\n",
    "print(f\"Taille du modèle : {size_quantized:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Pruned: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:44<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy du modèle pruned: 93.39%\n",
      "Taille du modèle pruned: 399.95 MB\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Créer une copie du modèle fine-tuné pour le pruning\n",
    "model_pruned = copy.deepcopy(model)\n",
    "\n",
    "# Appliquer du pruning sur la tête de classification\n",
    "prune.random_unstructured(model_pruned.classifier, name=\"weight\", amount=0.3)\n",
    "\n",
    "# Optionnel : Appliquer du pruning sur les Linear des 2 derniers blocs du transformeur\n",
    "for layer in model_pruned.vit.encoder.layer[-2:]:\n",
    "    for module in layer.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "\n",
    "# Évaluation du modèle pruned sur le jeu de test\n",
    "model_pruned.eval()\n",
    "correct_p = 0\n",
    "total_p = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluation Pruned\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_pruned(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct_p += (preds == labels.to(device)).sum().item()\n",
    "        total_p += labels.size(0)\n",
    "print(\"Accuracy du modèle pruned: {:.2f}%\".format(100 * correct_p / total_p))\n",
    "\n",
    "# Comparaison des tailles\n",
    "size_pruned = get_model_size(model_pruned)\n",
    "print(\"Taille du modèle pruned: {:.2f} MB\".format(size_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Pruned:   0%|                                                                                                                                                       | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Pruned: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [03:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📉 Résultats - PRUNING\n",
      "Accuracy : 94.96%\n",
      "Taille du modèle : 399.92 MB\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Copier le modèle fine-tuné\n",
    "model_pruned = copy.deepcopy(model)\n",
    "\n",
    "# Appliquer du pruning non structuré sur la tête de classification\n",
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "prune.remove(module, 'weight')  # Supprime le mask et applique le pruning définitivement\n",
    "\n",
    "# Optionnel : pruning sur les 2 derniers blocs ViT\n",
    "for layer in model_pruned.vit.encoder.layer[-2:]:\n",
    "    for module in layer.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "\n",
    "model_pruned.to(\"cpu\")\n",
    "\n",
    "# Évaluation du modèle pruned\n",
    "correct_p, total_p = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluation Pruned\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        outputs = model_pruned(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "        correct_p += (preds == labels).sum().item()\n",
    "        total_p += labels.size(0)\n",
    "\n",
    "accuracy_pruned = 100 * correct_p / total_p\n",
    "size_pruned = get_model_size(model_pruned)\n",
    "\n",
    "print(\"\\n📉 Résultats - PRUNING\")\n",
    "print(f\"Accuracy : {accuracy_pruned:.2f}%\")\n",
    "print(f\"Taille du modèle : {size_pruned:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARAISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 RÉCAPITULATIF DES MODÈLES\n",
      "\n",
      "┌──────────────────────────────┬────────────┬────────────┐\n",
      "│ Modèle                       │ Accuracy   │ Taille (MB)│\n",
      "├──────────────────────────────┼────────────┼────────────┤\n",
      "│ Baseline (fine-tuné)         │    97.24% │     343.29 │\n",
      "│ Quantized (tête uniquement)  │    97.24% │     343.29 │\n",
      "│ Pruned (tête + 2 derniers)   │    94.96% │     399.92 │\n",
      "└──────────────────────────────┴────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 RÉCAPITULATIF DES MODÈLES\\n\")\n",
    "print(\"┌──────────────────────────────┬────────────┬────────────┐\")\n",
    "print(\"│ Modèle                       │ Accuracy   │ Taille (MB)│\")\n",
    "print(\"├──────────────────────────────┼────────────┼────────────┤\")\n",
    "print(f\"│ Baseline (fine-tuné)         │ {100 * correct / total:8.2f}% │ {size_baseline:10.2f} │\")\n",
    "print(f\"│ Quantized (tête uniquement)  │ {accuracy_quantized:8.2f}% │ {size_quantized:10.2f} │\")\n",
    "print(f\"│ Pruned (tête + 2 derniers)   │ {accuracy_pruned:8.2f}% │ {size_pruned:10.2f} │\")\n",
    "print(\"└──────────────────────────────┴────────────┴────────────┘\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
